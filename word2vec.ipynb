{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819eaf45-dbe1-4e9a-9414-dbc429dbc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\", \"ner\"])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae8ea-bb8a-4388-9d91-56ca8b17e413",
   "metadata": {},
   "source": [
    "## Import pre-processed data\n",
    "\n",
    "Write a function to import pre-processed data for modelling. Currently, just reading from a previously saved csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    reg_punc =re.compile(r'[^\\w\\s]')\n",
    "    text = reg_punc.sub(r'', text)\n",
    "\n",
    "    # remove html\n",
    "    reg_html = re.compile(r'<.*?>')\n",
    "    text = reg_html.sub(r'', text)\n",
    "\n",
    "    # remove url\n",
    "    reg_url = re.compile(r'http\\S+')\n",
    "    text = reg_url.sub(r'', text)\n",
    "\n",
    "    # remove numerical values\n",
    "    reg_num = re.compile(r'[0-9]')\n",
    "    text = reg_num.sub(r'', text)\n",
    "\n",
    "    # remove special characters\n",
    "    reg_spcl = re.compile('[@_!#$%^&*()<>?/\\\\|}{~:]')\n",
    "    text = reg_spcl.sub(r'', text)\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_url = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_url.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414d250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dev_data.csv\")\n",
    "data['text'] = data['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "emotions = data['label'].unique().tolist()\n",
    "emotions.sort()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data['text'], data['label'], random_state=0,\n",
    "                                                   test_size= 0.3, stratify= data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55187df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    w2v_data = []\n",
    "    for tt in tqdm(X):\n",
    "        w2v_data.append([ww for ww in word_tokenize(tt.lower()) if ww not in stop_words])\n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f32c4509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caring', 0.8846673965454102),\n",
       " ('loves', 0.8031463623046875),\n",
       " ('love', 0.7764047980308533),\n",
       " ('gracious', 0.7666694521903992),\n",
       " ('trusting', 0.7659983038902283),\n",
       " ('supportive', 0.7656123042106628),\n",
       " ('compassionate', 0.7655137777328491),\n",
       " ('gods', 0.7332736849784851),\n",
       " ('affectionate', 0.7303548455238342),\n",
       " ('affection', 0.7222259640693665)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w2v_train = preprocess(X_train)\n",
    "# w2v_test = preprocess(X_test)\n",
    "\n",
    "# model = Word2Vec(w2v_train, min_count = 1, window = 3, vector_size= 50)\n",
    "model.wv.most_similar('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e6a417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(X):\n",
    "    # convert sentence to vectors by taking a simple average of all word embeddings\n",
    "    return [np.zeros(model.vector_size) if 0== len(vv) else model.wv.get_mean_vector(vv, pre_normalize= False) for vv in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c418c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:08<00:00, 29109.27it/s]\n",
      "100%|██████████| 99880/99880 [00:04<00:00, 21624.49it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v = sentence_to_vector(w2v_train)\n",
    "X_test_w2v = sentence_to_vector(w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5f19e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194726        joy\n",
      "149286    sadness\n",
      "108993        joy\n",
      "152321      anger\n",
      "251081       love\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bd0cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>love</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829350</td>\n",
       "      <td>0.822418</td>\n",
       "      <td>0.585457</td>\n",
       "      <td>0.704592</td>\n",
       "      <td>0.730838</td>\n",
       "      <td>0.793369</td>\n",
       "      <td>0.617241</td>\n",
       "      <td>0.739255</td>\n",
       "      <td>0.821148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.829350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>0.638296</td>\n",
       "      <td>0.769566</td>\n",
       "      <td>0.861654</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.831201</td>\n",
       "      <td>0.861148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.822418</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>0.853964</td>\n",
       "      <td>0.743277</td>\n",
       "      <td>0.758577</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.749143</td>\n",
       "      <td>0.831171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.585457</td>\n",
       "      <td>0.638296</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>0.786239</td>\n",
       "      <td>0.645865</td>\n",
       "      <td>0.554621</td>\n",
       "      <td>0.504831</td>\n",
       "      <td>0.767486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.704592</td>\n",
       "      <td>0.769566</td>\n",
       "      <td>0.853964</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735415</td>\n",
       "      <td>0.725132</td>\n",
       "      <td>0.821895</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.766302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.730838</td>\n",
       "      <td>0.861654</td>\n",
       "      <td>0.743277</td>\n",
       "      <td>0.786239</td>\n",
       "      <td>0.735415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.793655</td>\n",
       "      <td>0.754247</td>\n",
       "      <td>0.742215</td>\n",
       "      <td>0.853328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.793369</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.758577</td>\n",
       "      <td>0.645865</td>\n",
       "      <td>0.725132</td>\n",
       "      <td>0.793655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628537</td>\n",
       "      <td>0.761067</td>\n",
       "      <td>0.858748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.617241</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.554621</td>\n",
       "      <td>0.821895</td>\n",
       "      <td>0.754247</td>\n",
       "      <td>0.628537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807576</td>\n",
       "      <td>0.727844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.739255</td>\n",
       "      <td>0.831201</td>\n",
       "      <td>0.749143</td>\n",
       "      <td>0.504831</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.742215</td>\n",
       "      <td>0.761067</td>\n",
       "      <td>0.807576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.821148</td>\n",
       "      <td>0.861148</td>\n",
       "      <td>0.831171</td>\n",
       "      <td>0.767486</td>\n",
       "      <td>0.766302</td>\n",
       "      <td>0.853328</td>\n",
       "      <td>0.858748</td>\n",
       "      <td>0.727844</td>\n",
       "      <td>0.715168</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label         joy   sadness       joy     anger      love     anger   sadness  \\\n",
       "label                                                                           \n",
       "joy      1.000000  0.829350  0.822418  0.585457  0.704592  0.730838  0.793369   \n",
       "sadness  0.829350  1.000000  0.814826  0.638296  0.769566  0.861654  0.867700   \n",
       "joy      0.822418  0.814826  1.000000  0.550974  0.853964  0.743277  0.758577   \n",
       "anger    0.585457  0.638296  0.550974  1.000000  0.551553  0.786239  0.645865   \n",
       "love     0.704592  0.769566  0.853964  0.551553  1.000000  0.735415  0.725132   \n",
       "anger    0.730838  0.861654  0.743277  0.786239  0.735415  1.000000  0.793655   \n",
       "sadness  0.793369  0.867700  0.758577  0.645865  0.725132  0.793655  1.000000   \n",
       "anger    0.617241  0.784065  0.781829  0.554621  0.821895  0.754247  0.628537   \n",
       "sadness  0.739255  0.831201  0.749143  0.504831  0.768500  0.742215  0.761067   \n",
       "love     0.821148  0.861148  0.831171  0.767486  0.766302  0.853328  0.858748   \n",
       "\n",
       "label       anger   sadness      love  \n",
       "label                                  \n",
       "joy      0.617241  0.739255  0.821148  \n",
       "sadness  0.784065  0.831201  0.861148  \n",
       "joy      0.781829  0.749143  0.831171  \n",
       "anger    0.554621  0.504831  0.767486  \n",
       "love     0.821895  0.768500  0.766302  \n",
       "anger    0.754247  0.742215  0.853328  \n",
       "sadness  0.628537  0.761067  0.858748  \n",
       "anger    1.000000  0.807576  0.727844  \n",
       "sadness  0.807576  1.000000  0.715168  \n",
       "love     0.727844  0.715168  1.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(X_train_w2v[:10]), index = y_train[:10], columns= y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5761e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word2Vec Transformer\n",
    "class W2VEmbeddings(TransformerMixin):\n",
    "    def __init__(self, w2v_model= None, weights = None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.weights = weights\n",
    "        self.word2weight = None\n",
    "        # self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        w2v_X = preprocess(X)\n",
    "\n",
    "        if self.w2v_model is None:\n",
    "            print(\"Fitting new Word2Vec model on training data.\")\n",
    "            w2v_model = Word2Vec(w2v_X, min_count = 1, window = 3, vector_size= 100)\n",
    "            self.w2v_model = w2v_model\n",
    "            print(\"Done!\")\n",
    "            \n",
    "        if self.weights == \"idf\":\n",
    "            tfidf = TfidfVectorizer(analyzer= lambda x: x)\n",
    "            tfidf.fit(w2v_X)\n",
    "            # if a word was never seen - it must be at least as infrequent as any of the known words\n",
    "            # so the default idf is the max of known idf's\n",
    "            max_idf = max(tfidf.idf_)\n",
    "            self.word2weight = defaultdict(\n",
    "                lambda: max_idf,\n",
    "                [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "            print(\"Fit the IDF Model\")\n",
    "        else:\n",
    "            self.word2weight = defaultdict(lambda: 1)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_w2v = []\n",
    "        for vv in preprocess(X):\n",
    "            X_w2v.append(np.mean([self.w2v_model.wv[w] * self.word2weight[w] for w in vv if w in self.w2v_model.wv.key_to_index.keys()] \n",
    "                    or [np.zeros(model.vector_size)], axis= 0))\n",
    "        return X_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc693761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1809.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>love</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829350</td>\n",
       "      <td>0.822418</td>\n",
       "      <td>0.585457</td>\n",
       "      <td>0.704592</td>\n",
       "      <td>0.730838</td>\n",
       "      <td>0.793369</td>\n",
       "      <td>0.617241</td>\n",
       "      <td>0.739255</td>\n",
       "      <td>0.821148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.829350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>0.638296</td>\n",
       "      <td>0.769566</td>\n",
       "      <td>0.861654</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.831201</td>\n",
       "      <td>0.861148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.822418</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>0.853964</td>\n",
       "      <td>0.743277</td>\n",
       "      <td>0.758577</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.749143</td>\n",
       "      <td>0.831171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.585457</td>\n",
       "      <td>0.638296</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>0.786239</td>\n",
       "      <td>0.645865</td>\n",
       "      <td>0.554621</td>\n",
       "      <td>0.504831</td>\n",
       "      <td>0.767486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.704592</td>\n",
       "      <td>0.769566</td>\n",
       "      <td>0.853964</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735415</td>\n",
       "      <td>0.725132</td>\n",
       "      <td>0.821895</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.766302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.730838</td>\n",
       "      <td>0.861654</td>\n",
       "      <td>0.743277</td>\n",
       "      <td>0.786239</td>\n",
       "      <td>0.735415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.793655</td>\n",
       "      <td>0.754247</td>\n",
       "      <td>0.742215</td>\n",
       "      <td>0.853328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.793369</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.758577</td>\n",
       "      <td>0.645865</td>\n",
       "      <td>0.725132</td>\n",
       "      <td>0.793655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628537</td>\n",
       "      <td>0.761067</td>\n",
       "      <td>0.858748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.617241</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.554621</td>\n",
       "      <td>0.821895</td>\n",
       "      <td>0.754247</td>\n",
       "      <td>0.628537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807576</td>\n",
       "      <td>0.727844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.739255</td>\n",
       "      <td>0.831201</td>\n",
       "      <td>0.749143</td>\n",
       "      <td>0.504831</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.742215</td>\n",
       "      <td>0.761067</td>\n",
       "      <td>0.807576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.821148</td>\n",
       "      <td>0.861148</td>\n",
       "      <td>0.831171</td>\n",
       "      <td>0.767486</td>\n",
       "      <td>0.766302</td>\n",
       "      <td>0.853328</td>\n",
       "      <td>0.858748</td>\n",
       "      <td>0.727844</td>\n",
       "      <td>0.715168</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label         joy   sadness       joy     anger      love     anger   sadness  \\\n",
       "label                                                                           \n",
       "joy      1.000000  0.829350  0.822418  0.585457  0.704592  0.730838  0.793369   \n",
       "sadness  0.829350  1.000000  0.814826  0.638296  0.769566  0.861654  0.867700   \n",
       "joy      0.822418  0.814826  1.000000  0.550974  0.853964  0.743277  0.758577   \n",
       "anger    0.585457  0.638296  0.550974  1.000000  0.551553  0.786239  0.645865   \n",
       "love     0.704592  0.769566  0.853964  0.551553  1.000000  0.735415  0.725132   \n",
       "anger    0.730838  0.861654  0.743277  0.786239  0.735415  1.000000  0.793655   \n",
       "sadness  0.793369  0.867700  0.758577  0.645865  0.725132  0.793655  1.000000   \n",
       "anger    0.617241  0.784065  0.781829  0.554621  0.821895  0.754247  0.628537   \n",
       "sadness  0.739255  0.831201  0.749143  0.504831  0.768500  0.742215  0.761067   \n",
       "love     0.821148  0.861148  0.831171  0.767486  0.766302  0.853328  0.858748   \n",
       "\n",
       "label       anger   sadness      love  \n",
       "label                                  \n",
       "joy      0.617241  0.739255  0.821148  \n",
       "sadness  0.784065  0.831201  0.861148  \n",
       "joy      0.781829  0.749143  0.831171  \n",
       "anger    0.554621  0.504831  0.767486  \n",
       "love     0.821895  0.768500  0.766302  \n",
       "anger    0.754247  0.742215  0.853328  \n",
       "sadness  0.628537  0.761067  0.858748  \n",
       "anger    1.000000  0.807576  0.727844  \n",
       "sadness  0.807576  1.000000  0.715168  \n",
       "love     0.727844  0.715168  1.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w2v = W2VEmbeddings(weights= None, w2v_model= model).fit_transform(X_train[:10])\n",
    "pd.DataFrame(cosine_similarity(test_w2v), index = y_train[:10], columns= y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee216b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 200 ms\n",
      "{'n_neighbors': 100}\n",
      "CPU times: total: 5.59 s\n",
      "Wall time: 7.19 s\n",
      "KNN classification accuracy is: 0.49.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.48      0.01      0.03     13889\n",
      "        fear       0.59      0.02      0.04     11776\n",
      "         joy       0.56      0.81      0.66     33625\n",
      "        love       0.41      0.00      0.00      8144\n",
      "     sadness       0.43      0.75      0.55     28916\n",
      "    surprise       0.38      0.00      0.00      3530\n",
      "\n",
      "    accuracy                           0.49     99880\n",
      "   macro avg       0.48      0.27      0.21     99880\n",
      "weighted avg       0.50      0.49      0.39     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 10, metric= 'cosine', weights= 'distance')\n",
    "knn_grid = GridSearchCV(knn, cv= 3, scoring= 'accuracy', param_grid= {'n_neighbors':[1, 5, 10, 50, 100, 200]}, verbose= 1)\n",
    "%time knn_grid.fit(X_train_w2v[:1000], y_train.head(1000))\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "%time y_pred = knn_grid.best_estimator_.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de092ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 125 ms\n",
      "KNN classification accuracy is: 0.57.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.47      0.13      0.20     13889\n",
      "        fear       0.50      0.19      0.28     11776\n",
      "         joy       0.65      0.86      0.74     33625\n",
      "        love       0.53      0.06      0.10      8144\n",
      "     sadness       0.51      0.79      0.62     28916\n",
      "    surprise       0.21      0.12      0.15      3530\n",
      "\n",
      "    accuracy                           0.57     99880\n",
      "   macro avg       0.48      0.36      0.35     99880\n",
      "weighted avg       0.54      0.57      0.50     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SGDClassifier(loss= 'hinge')\n",
    "svm.fit(X_train_w2v, y_train)\n",
    "\n",
    "%time y_pred = svm.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b63814ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always want home place others feel welcomed loved comfortable',\n",
       " 'know like work always underlining feeling ignored forgotten',\n",
       " 'hate able make feel better',\n",
       " 'id memory feelings insincere',\n",
       " 'make mood feel horny',\n",
       " 'feel angry depressed work steal glance boss feelings dissipate',\n",
       " 'feel like hated person planet turning brendon',\n",
       " 'im feeling slightly agitated today cant assed put better mood',\n",
       " 'get stuff else end feeling lame sitting around house thumb butt',\n",
       " 'feel today going passionate day one want show one feelings situation whether romantic sense value either personally socially']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\" \".join(x) for x in w2v_train[:10]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "8baaca0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[299], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m      3\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# if a word was never seen - it must be at least as infrequent\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# as any of the known words - so the default idf is the max of \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# known idf's\u001b[39;00m\n\u001b[0;32m      8\u001b[0m max_idf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(tfidf\u001b[38;5;241m.\u001b[39midf_)\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2061\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2056\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2057\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2058\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2059\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2060\u001b[0m )\n\u001b[1;32m-> 2061\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2062\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\github\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:66\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 66\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(w2v_train[:10])\n",
    "# if a word was never seen - it must be at least as infrequent\n",
    "# as any of the known words - so the default idf is the max of \n",
    "# known idf's\n",
    "max_idf = max(tfidf.idf_)\n",
    "word2weight = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "pd.DataFrame.from_dict({word:tfidf.idf_[i] for word,i in tfidf.vocabulary_.items()}, orient= 'index').sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a832224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7047480922384253"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2weight['amateur']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ece5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
