{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "819eaf45-dbe1-4e9a-9414-dbc429dbc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae8ea-bb8a-4388-9d91-56ca8b17e413",
   "metadata": {},
   "source": [
    "## Import pre-processed data\n",
    "\n",
    "Write a function to import pre-processed data for modelling. Currently, just reading from a previously saved csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "509299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    reg_punc =re.compile(r'[^\\w\\s]')\n",
    "    text = reg_punc.sub(r'', text)\n",
    "\n",
    "    # remove html\n",
    "    reg_html = re.compile(r'<.*?>')\n",
    "    text = reg_html.sub(r'', text)\n",
    "\n",
    "    # remove url\n",
    "    reg_url = re.compile(r'http\\S+')\n",
    "    text = reg_url.sub(r'', text)\n",
    "\n",
    "    # remove numerical values\n",
    "    reg_num = re.compile(r'[0-9]')\n",
    "    text = reg_num.sub(r'', text)\n",
    "\n",
    "    # remove special characters\n",
    "    reg_spcl = re.compile('[@_!#$%^&*()<>?/\\\\|}{~:]')\n",
    "    text = reg_spcl.sub(r'', text)\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_url = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_url.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "414d250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dev_data.csv\")\n",
    "data['text'] = data['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "emotions = data['label'].unique().tolist()\n",
    "emotions.sort()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "934bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data['text'], data['label'], random_state=0,\n",
    "                                                   test_size= 0.3, stratify= data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55187df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    w2v_data = []\n",
    "    for tt in tqdm(X):\n",
    "        w2v_data.append([ww for ww in word_tokenize(tt.lower()) if ww not in stop_words])\n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f32c4509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:34<00:00, 6814.44it/s]\n",
      "100%|██████████| 99880/99880 [00:13<00:00, 7233.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('loving', 0.7861563563346863),\n",
       " ('miss', 0.7359917759895325),\n",
       " ('loves', 0.7110063433647156),\n",
       " ('loved', 0.6903106570243835),\n",
       " ('passion', 0.6851310729980469),\n",
       " ('caring', 0.684219241142273),\n",
       " ('spirit', 0.6817525029182434),\n",
       " ('thank', 0.6717989444732666),\n",
       " ('joy', 0.6585392951965332),\n",
       " ('jesus', 0.6555297374725342)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train = preprocess(X_train)\n",
    "w2v_test = preprocess(X_test)\n",
    "\n",
    "model = Word2Vec(w2v_train, min_count = 1, window = 3, vector_size= 50)\n",
    "model.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1e6a417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(X):\n",
    "    # convert sentence to vectors by taking a simple average of all word embeddings\n",
    "    X_w2v = []\n",
    "    for vv in tqdm(X):\n",
    "        try:\n",
    "            X_w2v.append(model.wv.get_mean_vector(vv))\n",
    "        except:\n",
    "            X_w2v.append(np.zeros(model.wv.get_mean_vector(['anger']).shape))\n",
    "    return X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c418c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:10<00:00, 21995.88it/s]\n",
      "100%|██████████| 99880/99880 [00:06<00:00, 16645.13it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v = sentence_to_vector(w2v_train)\n",
    "X_test_w2v = sentence_to_vector(w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c5761e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word2Vec Transformer\n",
    "class W2VEmbeddings(TransformerMixin):\n",
    "    def __init__(self, w2v_model= None):\n",
    "        self.w2v_model = w2v_model\n",
    "        # self.word2weight = None\n",
    "        # self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        w2v_X = preprocess(X)\n",
    "        w2v_model = Word2Vec(w2v_X, min_count = 1, window = 3, vector_size= 50)\n",
    "        self.w2v_model = w2v_model\n",
    "        print(self.w2v_model.wv.most_similar('love'))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_w2v = []\n",
    "        for vv in preprocess(X):\n",
    "            try:\n",
    "                X_w2v.append(self.w2v_model.wv.get_mean_vector(vv))\n",
    "            except:\n",
    "                X_w2v.append(np.zeros(self.w2v_model.wv.get_mean_vector(['anger']).shape))\n",
    "        return X_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cc693761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:46<00:00, 5048.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('loving', 0.7673147320747375), ('miss', 0.7473839521408081), ('loved', 0.7119632363319397), ('loves', 0.7029842734336853), ('caring', 0.6805114150047302), ('passion', 0.6768738031387329), ('thalli', 0.6639769077301025), ('thank', 0.6580396890640259), ('hope', 0.6568820476531982), ('ls', 0.6539918780326843)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['always', 'want', 'home', 'place', 'others', 'feel', 'welcomed', 'loved', 'comfortable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.01708964,  0.01921386,  0.01527537,  0.07565784, -0.07027525,\n",
       "        -0.14357822,  0.05914228,  0.1138384 , -0.11386105, -0.18942688,\n",
       "         0.07240799, -0.17234784, -0.06502836,  0.08738159, -0.08316305,\n",
       "        -0.02276425,  0.07295597,  0.07020946, -0.12652834, -0.10044847,\n",
       "         0.0821257 ,  0.03505055,  0.00377918,  0.04587409, -0.02133356,\n",
       "         0.08336064, -0.0814569 ,  0.0871041 , -0.11599946,  0.15503816,\n",
       "         0.13451521, -0.16397108,  0.0215542 , -0.00796315, -0.07336248,\n",
       "         0.15308338,  0.06939812,  0.1335659 ,  0.09177619, -0.00153498,\n",
       "         0.2505225 , -0.04006877, -0.11877075,  0.1285692 ,  0.09433106,\n",
       "         0.05336119,  0.00206748,  0.00461212,  0.19667767, -0.09340246],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w2v = W2VEmbeddings().fit(X_train)\n",
    "test_w2v.transform(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b3bbd46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "4f59716d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv = w2v_train[11825]\n",
    "\n",
    "np.mean([model.wv[w] for w in vv if w in model.wv.key_to_index.keys()] or [np.zeros(model.vector_size)], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ee216b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "CPU times: total: 4.66 s\n",
      "Wall time: 7.06 s\n",
      "{'n_neighbors': 100}\n",
      "CPU times: total: 26.5 s\n",
      "Wall time: 32.5 s\n",
      "KNN classification accuracy is: 0.53.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.08      0.13     13889\n",
      "        fear       0.60      0.10      0.17     11776\n",
      "         joy       0.58      0.84      0.69     33625\n",
      "        love       0.57      0.04      0.07      8144\n",
      "     sadness       0.47      0.76      0.58     28916\n",
      "    surprise       0.33      0.00      0.01      3530\n",
      "\n",
      "    accuracy                           0.53     99880\n",
      "   macro avg       0.52      0.30      0.27     99880\n",
      "weighted avg       0.54      0.53      0.44     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 10, metric= 'cosine', weights= 'distance')\n",
    "knn_grid = GridSearchCV(knn, cv= 3, scoring= 'accuracy', param_grid= {'n_neighbors':[1, 5, 10, 50, 100, 200]}, verbose= 1)\n",
    "%time knn_grid.fit(X_train_w2v[:50000], y_train.head(50000))\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "%time y_pred = knn_grid.best_estimator_.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "de092ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 50 ms\n",
      "KNN classification accuracy is: 0.54.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.10      0.17     13889\n",
      "        fear       0.72      0.09      0.15     11776\n",
      "         joy       0.59      0.89      0.71     33625\n",
      "        love       0.26      0.07      0.11      8144\n",
      "     sadness       0.53      0.72      0.61     28916\n",
      "    surprise       0.14      0.12      0.13      3530\n",
      "\n",
      "    accuracy                           0.54     99880\n",
      "   macro avg       0.46      0.33      0.31     99880\n",
      "weighted avg       0.54      0.54      0.47     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SGDClassifier(loss= 'hinge')\n",
    "svm.fit(X_train_w2v, y_train)\n",
    "\n",
    "%time y_pred = svm.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b63814ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always want home place others feel welcomed loved comfortable',\n",
       " 'know like work always underlining feeling ignored forgotten',\n",
       " 'hate able make feel better',\n",
       " 'id memory feelings insincere',\n",
       " 'make mood feel horny',\n",
       " 'feel angry depressed work steal glance boss feelings dissipate',\n",
       " 'feel like hated person planet turning brendon',\n",
       " 'im feeling slightly agitated today cant assed put better mood',\n",
       " 'get stuff else end feeling lame sitting around house thumb butt',\n",
       " 'feel today going passionate day one want show one feelings situation whether romantic sense value either personally socially']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\" \".join(x) for x in w2v_train[:10]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8baaca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>1.451985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelings</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeling</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personally</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planet</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turning</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socially</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "feel        1.451985\n",
       "feelings    2.011601\n",
       "feeling     2.011601\n",
       "always      2.299283\n",
       "today       2.299283\n",
       "...              ...\n",
       "personally  2.704748\n",
       "planet      2.704748\n",
       "turning     2.704748\n",
       "memory      2.704748\n",
       "socially    2.704748\n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(w2v_train[:10])\n",
    "# if a word was never seen - it must be at least as infrequent\n",
    "# as any of the known words - so the default idf is the max of \n",
    "# known idf's\n",
    "max_idf = max(tfidf.idf_)\n",
    "word2weight = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "pd.DataFrame.from_dict({word:tfidf.idf_[i] for word,i in tfidf.vocabulary_.items()}, orient= 'index').sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a832224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7047480922384253"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2weight['amateur']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ece5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
