{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "819eaf45-dbe1-4e9a-9414-dbc429dbc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\", \"ner\"])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae8ea-bb8a-4388-9d91-56ca8b17e413",
   "metadata": {},
   "source": [
    "## Import pre-processed data\n",
    "\n",
    "Write a function to import pre-processed data for modelling. Currently, just reading from a previously saved csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    reg_punc =re.compile(r'[^\\w\\s]')\n",
    "    text = reg_punc.sub(r'', text)\n",
    "\n",
    "    # remove html\n",
    "    reg_html = re.compile(r'<.*?>')\n",
    "    text = reg_html.sub(r'', text)\n",
    "\n",
    "    # remove url\n",
    "    reg_url = re.compile(r'http\\S+')\n",
    "    text = reg_url.sub(r'', text)\n",
    "\n",
    "    # remove numerical values\n",
    "    reg_num = re.compile(r'[0-9]')\n",
    "    text = reg_num.sub(r'', text)\n",
    "\n",
    "    # remove special characters\n",
    "    reg_spcl = re.compile('[@_!#$%^&*()<>?/\\\\|}{~:]')\n",
    "    text = reg_spcl.sub(r'', text)\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_url = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_url.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414d250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dev_data.csv\")\n",
    "data['text'] = data['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "emotions = data['label'].unique().tolist()\n",
    "emotions.sort()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data['text'], data['label'], random_state=0,\n",
    "                                                   test_size= 0.3, stratify= data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "55187df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    w2v_data = []\n",
    "    for tt in X:\n",
    "        w2v_data.append([ww for ww in word_tokenize(tt.lower()) if ww not in stop_words])\n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f32c4509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compassionate', 0.9249340295791626),\n",
       " ('hostile', 0.9108782410621643),\n",
       " ('sympathetic', 0.8977436423301697),\n",
       " ('benevolent', 0.8855224251747131),\n",
       " ('considerate', 0.8787329792976379),\n",
       " ('spiteful', 0.8751549124717712),\n",
       " ('hateful', 0.8732966780662537),\n",
       " ('unfriendly', 0.8729376196861267),\n",
       " ('submissive', 0.8725266456604004),\n",
       " ('needy', 0.853763222694397)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w2v_train = preprocess(X_train)\n",
    "# w2v_test = preprocess(X_test)\n",
    "\n",
    "model = Word2Vec(w2v_train, min_count = 1, window = 2, vector_size= 300)\n",
    "model.wv.most_similar('affectionate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d9d48d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affection', 'affectionate', 'affectionately']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('playful', 0.6595738530158997),\n",
       " ('loving', 0.6079658269882202),\n",
       " ('endearing', 0.6056519746780396),\n",
       " ('affection', 0.5745200514793396),\n",
       " ('lovable', 0.5493666529655457),\n",
       " ('gentle', 0.5466917753219604),\n",
       " ('good_natured', 0.5455296039581299),\n",
       " ('charming', 0.5257826447486877),\n",
       " ('sarcastic', 0.523388147354126),\n",
       " ('easygoing', 0.5212149620056152)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog_wordvecs = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=50000)\n",
    "print([key for key, value in goog_wordvecs.key_to_index.items() if 'affection' in key])\n",
    "goog_wordvecs.most_similar('affectionate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c5761e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word2Vec Transformer\n",
    "class W2VEmbeddings(TransformerMixin):\n",
    "    def __init__(self, w2v_model= None, weights = None, max_len= None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.weights = weights\n",
    "        self.word2weight = None\n",
    "\n",
    "        # add max len parameter\n",
    "        if max_len is not None:\n",
    "            self.max_len = max_len\n",
    "        elif self.w2v_model is not None:\n",
    "            self.max_len = self.w2v_model.vector_size\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        w2v_X = preprocess(X)\n",
    "\n",
    "        if self.w2v_model is None:\n",
    "            print(\"Fitting new Word2Vec model on training data.\")\n",
    "            w2v_model = Word2Vec(w2v_X, min_count = 1, window = 3, vector_size= 100)\n",
    "            self.w2v_model = w2v_model.wv\n",
    "            self.max_len = self.w2v_model.vector_size\n",
    "            print(\"Done!\")\n",
    "            \n",
    "        if self.weights == \"idf\":\n",
    "            tfidf = TfidfVectorizer(analyzer= lambda x: x)\n",
    "            tfidf.fit(w2v_X)\n",
    "            # if a word was never seen - it must be at least as infrequent as any of the known words\n",
    "            # so the default idf is the max of known idf's\n",
    "            max_idf = max(tfidf.idf_)\n",
    "            self.word2weight = defaultdict(\n",
    "                lambda: max_idf,\n",
    "                [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "            print(\"Fit the IDF Model\")\n",
    "        else:\n",
    "            self.word2weight = defaultdict(lambda: 1)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_w2v = []\n",
    "        for vv in preprocess(X):\n",
    "            X_w2v.append(np.mean([self.w2v_model[w] * self.word2weight[w] for w in vv if w in self.w2v_model.key_to_index.keys()] \n",
    "                    or [np.zeros(self.max_len)], axis= 0))\n",
    "        return X_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cc693761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the IDF Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>love</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.662850</td>\n",
       "      <td>0.704730</td>\n",
       "      <td>0.228687</td>\n",
       "      <td>0.566304</td>\n",
       "      <td>0.507530</td>\n",
       "      <td>0.583275</td>\n",
       "      <td>0.541933</td>\n",
       "      <td>0.546620</td>\n",
       "      <td>0.711151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.662850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.616139</td>\n",
       "      <td>0.296046</td>\n",
       "      <td>0.499304</td>\n",
       "      <td>0.555629</td>\n",
       "      <td>0.574476</td>\n",
       "      <td>0.510531</td>\n",
       "      <td>0.581215</td>\n",
       "      <td>0.644067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.704730</td>\n",
       "      <td>0.616139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246109</td>\n",
       "      <td>0.588937</td>\n",
       "      <td>0.535538</td>\n",
       "      <td>0.587511</td>\n",
       "      <td>0.638521</td>\n",
       "      <td>0.578051</td>\n",
       "      <td>0.698555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.228687</td>\n",
       "      <td>0.296046</td>\n",
       "      <td>0.246109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314674</td>\n",
       "      <td>0.364420</td>\n",
       "      <td>0.283305</td>\n",
       "      <td>0.385081</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.400822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.566304</td>\n",
       "      <td>0.499304</td>\n",
       "      <td>0.588937</td>\n",
       "      <td>0.314674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566616</td>\n",
       "      <td>0.425932</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.492389</td>\n",
       "      <td>0.630707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.507530</td>\n",
       "      <td>0.555629</td>\n",
       "      <td>0.535538</td>\n",
       "      <td>0.364420</td>\n",
       "      <td>0.566616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.502843</td>\n",
       "      <td>0.602613</td>\n",
       "      <td>0.542405</td>\n",
       "      <td>0.592260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.583275</td>\n",
       "      <td>0.574476</td>\n",
       "      <td>0.587511</td>\n",
       "      <td>0.283305</td>\n",
       "      <td>0.425932</td>\n",
       "      <td>0.502843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.497679</td>\n",
       "      <td>0.605178</td>\n",
       "      <td>0.646762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.541933</td>\n",
       "      <td>0.510531</td>\n",
       "      <td>0.638521</td>\n",
       "      <td>0.385081</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.602613</td>\n",
       "      <td>0.497679</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623918</td>\n",
       "      <td>0.598154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.546620</td>\n",
       "      <td>0.581215</td>\n",
       "      <td>0.578051</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.492389</td>\n",
       "      <td>0.542405</td>\n",
       "      <td>0.605178</td>\n",
       "      <td>0.623918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.711151</td>\n",
       "      <td>0.644067</td>\n",
       "      <td>0.698555</td>\n",
       "      <td>0.400822</td>\n",
       "      <td>0.630707</td>\n",
       "      <td>0.592260</td>\n",
       "      <td>0.646762</td>\n",
       "      <td>0.598154</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label         joy   sadness       joy     anger      love     anger   sadness  \\\n",
       "label                                                                           \n",
       "joy      1.000000  0.662850  0.704730  0.228687  0.566304  0.507530  0.583275   \n",
       "sadness  0.662850  1.000000  0.616139  0.296046  0.499304  0.555629  0.574476   \n",
       "joy      0.704730  0.616139  1.000000  0.246109  0.588937  0.535538  0.587511   \n",
       "anger    0.228687  0.296046  0.246109  1.000000  0.314674  0.364420  0.283305   \n",
       "love     0.566304  0.499304  0.588937  0.314674  1.000000  0.566616  0.425932   \n",
       "anger    0.507530  0.555629  0.535538  0.364420  0.566616  1.000000  0.502843   \n",
       "sadness  0.583275  0.574476  0.587511  0.283305  0.425932  0.502843  1.000000   \n",
       "anger    0.541933  0.510531  0.638521  0.385081  0.660200  0.602613  0.497679   \n",
       "sadness  0.546620  0.581215  0.578051  0.298118  0.492389  0.542405  0.605178   \n",
       "love     0.711151  0.644067  0.698555  0.400822  0.630707  0.592260  0.646762   \n",
       "\n",
       "label       anger   sadness      love  \n",
       "label                                  \n",
       "joy      0.541933  0.546620  0.711151  \n",
       "sadness  0.510531  0.581215  0.644067  \n",
       "joy      0.638521  0.578051  0.698555  \n",
       "anger    0.385081  0.298118  0.400822  \n",
       "love     0.660200  0.492389  0.630707  \n",
       "anger    0.602613  0.542405  0.592260  \n",
       "sadness  0.497679  0.605178  0.646762  \n",
       "anger    1.000000  0.623918  0.598154  \n",
       "sadness  0.623918  1.000000  0.602339  \n",
       "love     0.598154  0.602339  1.000000  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w2v = W2VEmbeddings(weights= 'idf', w2v_model= goog_wordvecs).fit_transform(X_train[:10])\n",
    "pd.DataFrame(cosine_similarity(test_w2v), index = y_train[:10], columns= y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee216b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:00<00:00, 5753.35it/s]\n",
      "100%|██████████| 666/666 [00:00<00:00, 9495.12it/s]\n",
      "100%|██████████| 334/334 [00:00<00:00, 10268.78it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 10443.06it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 6036.88it/s]\n",
      "100%|██████████| 333/333 [00:00<00:00, 8192.38it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 8139.64it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 8015.08it/s]\n",
      "100%|██████████| 333/333 [00:00<00:00, 7914.05it/s]\n",
      "100%|██████████| 666/666 [00:00<00:00, 8019.13it/s]\n",
      "100%|██████████| 666/666 [00:00<00:00, 8706.51it/s]\n",
      "100%|██████████| 334/334 [00:00<00:00, 8819.22it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 9993.93it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 9217.64it/s]\n",
      "100%|██████████| 333/333 [00:00<00:00, 9994.01it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 8578.73it/s]\n",
      "100%|██████████| 667/667 [00:00<00:00, 9823.04it/s]\n",
      "100%|██████████| 333/333 [00:00<00:00, 10192.91it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 6879.47it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8476.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 422 ms\n",
      "Wall time: 2.28 s\n",
      "{'svc__alpha': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99880/99880 [00:13<00:00, 7436.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.94 s\n",
      "Wall time: 18 s\n",
      "SVM accuracy is: 0.57.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.29      0.36     13889\n",
      "        fear       0.55      0.43      0.49     11776\n",
      "         joy       0.60      0.79      0.68     33625\n",
      "        love       0.45      0.18      0.25      8144\n",
      "     sadness       0.56      0.65      0.60     28916\n",
      "    surprise       0.62      0.28      0.38      3530\n",
      "\n",
      "    accuracy                           0.57     99880\n",
      "   macro avg       0.54      0.44      0.46     99880\n",
      "weighted avg       0.56      0.57      0.55     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_svm = Pipeline([\n",
    "    ('w2v', W2VEmbeddings(weights= None, w2v_model= goog_wordvecs)),\n",
    "    ('svc', SGDClassifier(loss= 'hinge'))\n",
    "])\n",
    "svm_grid = GridSearchCV(pipe_svm, cv= 3, scoring= 'accuracy', param_grid= {'svc__alpha':[0.001, 0.01]}, verbose= 1)\n",
    "%time svm_grid.fit(X_train[:1000], y_train.head(1000))\n",
    "print(svm_grid.best_params_)\n",
    "\n",
    "%time y_pred = svm_grid.best_estimator_.predict(X_test)\n",
    "\n",
    "print(f\"SVM accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2f34eb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 8761.05it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 9852.53it/s]\n",
      "100%|██████████| 99880/99880 [00:14<00:00, 6887.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification accuracy is: 0.63.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.48      0.54     13889\n",
      "        fear       0.59      0.49      0.53     11776\n",
      "         joy       0.67      0.75      0.70     33625\n",
      "        love       0.55      0.37      0.44      8144\n",
      "     sadness       0.61      0.73      0.66     28916\n",
      "    surprise       0.63      0.30      0.40      3530\n",
      "\n",
      "    accuracy                           0.63     99880\n",
      "   macro avg       0.61      0.52      0.55     99880\n",
      "weighted avg       0.62      0.63      0.62     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('w2v', W2VEmbeddings(weights= None, w2v_model= goog_wordvecs)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors= 10, metric= 'cosine', weights= 'distance'))\n",
    "])\n",
    "pipe_knn.fit(X_train[:10000], y_train.head(10000))\n",
    "\n",
    "y_pred = pipe_knn.predict(X_test)\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ece5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
