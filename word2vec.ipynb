{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "819eaf45-dbe1-4e9a-9414-dbc429dbc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae8ea-bb8a-4388-9d91-56ca8b17e413",
   "metadata": {},
   "source": [
    "## Import pre-processed data\n",
    "\n",
    "Write a function to import pre-processed data for modelling. Currently, just reading from a previously saved csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "509299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    reg_punc =re.compile(r'[^\\w\\s]')\n",
    "    text = reg_punc.sub(r'', text)\n",
    "\n",
    "    # remove html\n",
    "    reg_html = re.compile(r'<.*?>')\n",
    "    text = reg_html.sub(r'', text)\n",
    "\n",
    "    # remove url\n",
    "    reg_url = re.compile(r'http\\S+')\n",
    "    text = reg_url.sub(r'', text)\n",
    "\n",
    "    # remove numerical values\n",
    "    reg_num = re.compile(r'[0-9]')\n",
    "    text = reg_num.sub(r'', text)\n",
    "\n",
    "    # remove special characters\n",
    "    reg_spcl = re.compile('[@_!#$%^&*()<>?/\\\\|}{~:]')\n",
    "    text = reg_spcl.sub(r'', text)\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_url = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_url.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "414d250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dev_data.csv\")\n",
    "data['text'] = data['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "emotions = data['label'].unique().tolist()\n",
    "emotions.sort()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "934bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data['text'], data['label'], random_state=0,\n",
    "                                                   test_size= 0.3, stratify= data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55187df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    w2v_data = []\n",
    "    for tt in tqdm(X):\n",
    "        w2v_data.append([ww for ww in word_tokenize(tt.lower()) if ww not in stop_words])\n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f32c4509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:34<00:00, 6814.44it/s]\n",
      "100%|██████████| 99880/99880 [00:13<00:00, 7233.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('loving', 0.7861563563346863),\n",
       " ('miss', 0.7359917759895325),\n",
       " ('loves', 0.7110063433647156),\n",
       " ('loved', 0.6903106570243835),\n",
       " ('passion', 0.6851310729980469),\n",
       " ('caring', 0.684219241142273),\n",
       " ('spirit', 0.6817525029182434),\n",
       " ('thank', 0.6717989444732666),\n",
       " ('joy', 0.6585392951965332),\n",
       " ('jesus', 0.6555297374725342)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train = preprocess(X_train)\n",
    "w2v_test = preprocess(X_test)\n",
    "\n",
    "model = Word2Vec(w2v_train, min_count = 1, window = 3, vector_size= 50)\n",
    "model.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1e6a417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(X):\n",
    "    # convert sentence to vectors by taking a simple average of all word embeddings\n",
    "    X_w2v = []\n",
    "    for vv in tqdm(X):\n",
    "        try:\n",
    "            X_w2v.append(model.wv.get_mean_vector(vv))\n",
    "        except:\n",
    "            X_w2v.append(np.zeros(model.wv.get_mean_vector(['anger']).shape))\n",
    "    return X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c418c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233053/233053 [00:10<00:00, 21995.88it/s]\n",
      "100%|██████████| 99880/99880 [00:06<00:00, 16645.13it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v = sentence_to_vector(w2v_train)\n",
    "X_test_w2v = sentence_to_vector(w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c5761e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word2Vec Transformer\n",
    "class W2VEmbeddings(TransformerMixin):\n",
    "    def __init__(self, w2v_model= None):\n",
    "        self.w2v_model = w2v_model\n",
    "        # self.word2weight = None\n",
    "        # self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        w2v_X = preprocess(X)\n",
    "        w2v_model = Word2Vec(w2v_X, min_count = 1, window = 3, vector_size= 50)\n",
    "        self.w2v_model = w2v_model\n",
    "        print(self.w2v_model.wv.most_similar('love'))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X_w2v = []\n",
    "        for vv in preprocess(X):\n",
    "            print(vv)\n",
    "            X_w2v.append(np.mean([self.w2v_model.wv[w] for w in vv if w in self.w2v_model.wv.key_to_index.keys()] \n",
    "                    or [np.zeros(model.vector_size)], axis= 0))\n",
    "        return X_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d4bd9d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194726    i always want our home to be a place where oth...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cc693761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:08<00:00, 6224.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('friends', 0.9595770835876465), ('supporting', 0.9590832591056824), ('god', 0.9581246972084045), ('share', 0.9572946429252625), ('accepted', 0.9559061527252197), ('loved', 0.9543684720993042), ('caring', 0.9524101614952087), ('live', 0.9520652294158936), ('special', 0.9519128203392029), ('sincere', 0.9513068795204163)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'best', 'feeling']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.07077488,  0.21535026,  0.1976145 ,  0.32435048, -0.38505396,\n",
       "        -0.93124455,  1.3812528 ,  1.052912  , -0.97332543, -0.85820675,\n",
       "         0.03818871, -1.188398  ,  0.6946807 ,  0.16509019, -1.4486612 ,\n",
       "         0.11223182,  1.0000077 , -0.22863756, -1.6671696 , -0.7130013 ,\n",
       "         0.55658495,  0.78586847,  0.95190483, -0.75732344,  0.7915918 ,\n",
       "         0.71529895, -0.9535775 ,  0.10080209, -0.968029  , -0.81822824,\n",
       "        -0.22225536, -0.2586255 ,  0.51767635,  0.81950474, -1.1094187 ,\n",
       "         0.9794965 ,  0.61075383, -0.13021822,  0.06925955, -0.50232595,\n",
       "         0.45609066, -0.349906  , -0.27678287, -0.0663076 ,  1.2981023 ,\n",
       "        -0.06319097, -0.7450393 , -0.18962996,  0.6082306 ,  0.70886546],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_w2v = W2VEmbeddings().fit(X_train[:50000])\n",
    "test_w2v.transform(['love is the best feeling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ee216b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "CPU times: total: 4.66 s\n",
      "Wall time: 7.06 s\n",
      "{'n_neighbors': 100}\n",
      "CPU times: total: 26.5 s\n",
      "Wall time: 32.5 s\n",
      "KNN classification accuracy is: 0.53.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.08      0.13     13889\n",
      "        fear       0.60      0.10      0.17     11776\n",
      "         joy       0.58      0.84      0.69     33625\n",
      "        love       0.57      0.04      0.07      8144\n",
      "     sadness       0.47      0.76      0.58     28916\n",
      "    surprise       0.33      0.00      0.01      3530\n",
      "\n",
      "    accuracy                           0.53     99880\n",
      "   macro avg       0.52      0.30      0.27     99880\n",
      "weighted avg       0.54      0.53      0.44     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 10, metric= 'cosine', weights= 'distance')\n",
    "knn_grid = GridSearchCV(knn, cv= 3, scoring= 'accuracy', param_grid= {'n_neighbors':[1, 5, 10, 50, 100, 200]}, verbose= 1)\n",
    "%time knn_grid.fit(X_train_w2v[:50000], y_train.head(50000))\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "%time y_pred = knn_grid.best_estimator_.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "de092ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 50 ms\n",
      "KNN classification accuracy is: 0.54.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.10      0.17     13889\n",
      "        fear       0.72      0.09      0.15     11776\n",
      "         joy       0.59      0.89      0.71     33625\n",
      "        love       0.26      0.07      0.11      8144\n",
      "     sadness       0.53      0.72      0.61     28916\n",
      "    surprise       0.14      0.12      0.13      3530\n",
      "\n",
      "    accuracy                           0.54     99880\n",
      "   macro avg       0.46      0.33      0.31     99880\n",
      "weighted avg       0.54      0.54      0.47     99880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SGDClassifier(loss= 'hinge')\n",
    "svm.fit(X_train_w2v, y_train)\n",
    "\n",
    "%time y_pred = svm.predict(X_test_w2v)\n",
    "\n",
    "print(f\"KNN classification accuracy is: {accuracy_score(y_test, y_pred):0.2f}.\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b63814ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always want home place others feel welcomed loved comfortable',\n",
       " 'know like work always underlining feeling ignored forgotten',\n",
       " 'hate able make feel better',\n",
       " 'id memory feelings insincere',\n",
       " 'make mood feel horny',\n",
       " 'feel angry depressed work steal glance boss feelings dissipate',\n",
       " 'feel like hated person planet turning brendon',\n",
       " 'im feeling slightly agitated today cant assed put better mood',\n",
       " 'get stuff else end feeling lame sitting around house thumb butt',\n",
       " 'feel today going passionate day one want show one feelings situation whether romantic sense value either personally socially']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\" \".join(x) for x in w2v_train[:10]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8baaca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>1.451985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelings</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feeling</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personally</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planet</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turning</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socially</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "feel        1.451985\n",
       "feelings    2.011601\n",
       "feeling     2.011601\n",
       "always      2.299283\n",
       "today       2.299283\n",
       "...              ...\n",
       "personally  2.704748\n",
       "planet      2.704748\n",
       "turning     2.704748\n",
       "memory      2.704748\n",
       "socially    2.704748\n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(w2v_train[:10])\n",
    "# if a word was never seen - it must be at least as infrequent\n",
    "# as any of the known words - so the default idf is the max of \n",
    "# known idf's\n",
    "max_idf = max(tfidf.idf_)\n",
    "word2weight = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "pd.DataFrame.from_dict({word:tfidf.idf_[i] for word,i in tfidf.vocabulary_.items()}, orient= 'index').sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a832224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7047480922384253"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2weight['amateur']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ece5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
